#!/usr/bin/env python3
"""
Centralized TensorFlow Configuration for M1/M2 Mac Safety

CRITICAL: This module MUST be imported before any TensorFlow usage
to prevent Metal plugin crashes and GPU configuration errors.
"""

import os
import warnings

# **CRITICAL: Set TensorFlow environment variables BEFORE importing**
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TensorFlow logging
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN optimizations
os.environ['TF_METAL_DEVICE_PLACEMENT'] = '0'  # **DISABLE automatic Metal placement**
os.environ['TF_DISABLE_MKL'] = '1'  # **DISABLE MKL to avoid conflicts**
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '1'  # **NEW: Force GPU memory growth**
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # **NEW: Disable CUDA to force CPU/Metal only**

# **NEW: Aggressive Metal prevention**
os.environ['TF_DISABLE_SEGMENT_REDUCTION_OP'] = '1'
os.environ['TF_DISABLE_MKL_SMALL_MATRIX_OPT'] = '1'
os.environ['MLX_METAL_DEBUG'] = '0'  # Disable Metal debugging

warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

def print_system_resources():
    """Print detailed system resource information"""
    import platform
    import psutil
    
    print("\n" + "="*60)
    print("üñ•Ô∏è  SYSTEM RESOURCE INFORMATION")
    print("="*60)
    
    # System info
    print(f"üíª Platform: {platform.platform()}")
    print(f"üèóÔ∏è  Architecture: {platform.machine()}")
    print(f"üß† CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical")
    
    # Memory info
    memory = psutil.virtual_memory()
    print(f"üíæ RAM: {memory.total / (1024**3):.1f} GB total, {memory.available / (1024**3):.1f} GB available")
    
    # Detect M1/M2 Mac
    if platform.machine() == 'arm64' and platform.system() == 'Darwin':
        print("üçé Apple Silicon (M1/M2) detected - optimized for ARM64")
    elif platform.machine() == 'x86_64':
        print("üñ•Ô∏è  Intel/AMD x86_64 architecture detected")
    
    print("="*60)

def print_tensorflow_device_info():
    """Print TensorFlow device configuration and availability"""
    try:
        import tensorflow as tf
        
        print("\n" + "="*60)
        print("üîß TENSORFLOW DEVICE CONFIGURATION")
        print("="*60)
        
        # TensorFlow version
        print(f"üì¶ TensorFlow Version: {tf.__version__}")
        
        # Check physical devices
        print("\nüì± Physical Devices:")
        try:
            physical_devices = tf.config.list_physical_devices()
            if physical_devices:
                for device in physical_devices:
                    print(f"   ‚úÖ {device}")
            else:
                print("   ‚ö†Ô∏è  No physical devices detected")
        except Exception as e:
            print(f"   ‚ùå Error getting physical devices: {e}")
        
        # Check GPU availability
        print("\nüéÆ GPU Status:")
        try:
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                print(f"   ‚úÖ {len(gpus)} GPU(s) detected:")
                for i, gpu in enumerate(gpus):
                    print(f"      GPU {i}: {gpu}")
                    
                # Check if GPUs are actually usable
                try:
                    tf.config.experimental.set_memory_growth(gpus[0], True)
                    print("   ‚úÖ GPU memory growth enabled")
                except Exception as gpu_config_error:
                    print(f"   ‚ö†Ô∏è  GPU configuration warning: {gpu_config_error}")
            else:
                print("   ‚ùå No GPUs detected or GPU access disabled")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  GPU check error: {e}")
        
        # Check current strategy
        print("\nüéØ Training Strategy:")
        try:
            strategy = tf.distribute.get_strategy()
            print(f"   üìä Strategy: {type(strategy).__name__}")
            print(f"   üî¢ Number of replicas: {strategy.num_replicas_in_sync}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Strategy error: {e}")
        
        # Check mixed precision
        print("\n‚ö° Performance Options:")
        try:
            policy = tf.keras.mixed_precision.global_policy()
            print(f"   üé≠ Mixed Precision Policy: {policy.name}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Mixed precision check error: {e}")
        
        print("="*60)
        
    except ImportError:
        print("\n‚ùå TensorFlow not available for device info")
    except Exception as e:
        print(f"\n‚ùå Error getting TensorFlow device info: {e}")

def get_current_device():
    """Get the current TensorFlow device being used"""
    try:
        import tensorflow as tf
        
        # Try to determine current device
        try:
            # Create a simple operation to see where it runs
            with tf.device(None):  # Let TF choose
                test_op = tf.constant([1.0])
                device_name = test_op.device
                
            if device_name:
                if 'GPU' in device_name:
                    return f"üéÆ GPU: {device_name}"
                elif 'CPU' in device_name:
                    return f"üñ•Ô∏è  CPU: {device_name}"
                else:
                    return f"ü§î Unknown: {device_name}"
            else:
                return "ü§∑ Device not specified"
                
        except Exception as device_error:
            # Fallback: check available devices
            try:
                gpus = tf.config.list_physical_devices('GPU')
                if gpus and len(gpus) > 0:
                    return f"üéÆ GPU Available: {len(gpus)} device(s)"
                else:
                    return "üñ•Ô∏è  CPU (No GPU available)"
            except:
                return "üñ•Ô∏è  CPU (Fallback mode)"
                
    except ImportError:
        return "‚ùå TensorFlow not available"
    except Exception as e:
        return f"‚ùå Device detection error: {e}"

def print_training_device_info():
    """Print comprehensive device info before training starts"""
    print("\n" + "üöÄ" + "="*58 + "üöÄ")
    print("üéØ TRAINING RESOURCE ALLOCATION")
    print("üöÄ" + "="*58 + "üöÄ")
    
    # System resources
    print_system_resources()
    
    # TensorFlow devices
    print_tensorflow_device_info()
    
    # Current device being used
    current_device = get_current_device()
    print(f"\nüéØ CURRENT TRAINING DEVICE: {current_device}")
    
    print("\n" + "üöÄ" + "="*58 + "üöÄ")

def monitor_training_resources():
    """Monitor resource usage during training"""
    import psutil
    
    try:
        # CPU usage
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # Memory usage
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used_gb = (memory.total - memory.available) / (1024**3)
        memory_total_gb = memory.total / (1024**3)
        
        # TensorFlow device
        current_device = get_current_device()
        
        print(f"üìä Resource Usage: CPU {cpu_percent:.1f}% | "
              f"RAM {memory_used_gb:.1f}/{memory_total_gb:.1f} GB ({memory_percent:.1f}%) | "
              f"Device: {current_device.split(': ')[-1] if ': ' in current_device else current_device}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Resource monitoring error: {e}")

def is_tensorflow_available():
    """Check if TensorFlow is available and can be imported safely"""
    try:
        import tensorflow as tf
        # Basic functionality test with CPU enforcement
        with tf.device('/CPU:0'):
            _ = tf.constant([1.0, 2.0, 3.0])
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è TensorFlow not available: {e}")
        return False

def get_tensorflow():
    """Get TensorFlow with intelligent GPU/CPU selection"""
    try:
        import tensorflow as tf
        
        print("üîß Configuring TensorFlow with intelligent device selection...")
        
        # **STEP 1: Check GPU availability first**
        gpu_available = False
        try:
            gpus = tf.config.list_physical_devices('GPU')
            if gpus and len(gpus) > 0:
                gpu_available = True
                print(f"üéÆ {len(gpus)} GPU(s) detected: {[gpu.name for gpu in gpus]}")
                
                # **STEP 2: Try to configure GPU safely**
                try:
                    # Enable memory growth for all GPUs
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                    print("‚úÖ GPU memory growth enabled")
                    
                    # **Test GPU functionality**
                    with tf.device('/GPU:0'):
                        test_tensor = tf.constant([1., 2., 3.])
                        test_result = tf.reduce_sum(test_tensor).numpy()
                        print(f"‚úÖ GPU test successful: {test_result}")
                        print("üéÆ GPU mode activated!")
                        
                    return tf  # Return with GPU enabled
                    
                except Exception as gpu_error:
                    print(f"‚ö†Ô∏è GPU configuration failed: {gpu_error}")
                    print("üîÑ Falling back to CPU mode...")
                    gpu_available = False
            else:
                print("üñ•Ô∏è No GPUs detected")
                
        except Exception as gpu_check_error:
            print(f"‚ö†Ô∏è GPU detection error: {gpu_check_error}")
            print("üîÑ Falling back to CPU mode...")
        
        # **STEP 3: Configure for CPU mode (if no GPU or GPU failed)**
        if not gpu_available:
            print("üñ•Ô∏è Configuring TensorFlow for CPU-only mode...")
            
            try:
                # Disable GPU access
                tf.config.set_visible_devices([], 'GPU')
                print("‚úÖ GPU access disabled - CPU-only mode")
            except Exception as gpu_disable_error:
                print(f"‚ö†Ô∏è GPU disable warning: {gpu_disable_error}")
            
            # **Conservative threading for CPU**
            try:
                tf.config.threading.set_inter_op_parallelism_threads(2)
                tf.config.threading.set_intra_op_parallelism_threads(4)
                print("‚úÖ CPU threading optimized")
            except Exception as threading_error:
                print(f"‚ö†Ô∏è Threading config warning: {threading_error}")
        
        # **STEP 4: Set general TensorFlow optimizations**
        try:
            # Enable mixed precision for better performance (if supported)
            if gpu_available:
                try:
                    tf.keras.mixed_precision.set_global_policy('mixed_float16')
                    print("‚úÖ Mixed precision enabled (GPU)")
                except:
                    print("‚ö†Ô∏è Mixed precision not supported")
            
            # Set device policy
            tf.config.experimental.set_device_policy('silent_for_int32')
            print("‚úÖ Device policy configured")
            
        except Exception as config_error:
            print(f"‚ö†Ô∏è General config warning: {config_error}")
        
        # **STEP 5: Final functionality test**
        try:
            test_tensor = tf.constant([1., 2., 3.])
            test_computation = tf.reduce_sum(test_tensor)
            final_result = test_computation.numpy()
            
            # Determine which device was actually used
            device_name = test_tensor.device
            if 'GPU' in device_name:
                print(f"üéÆ TensorFlow configured successfully with GPU: {final_result}")
            else:
                print(f"üñ•Ô∏è TensorFlow configured successfully with CPU: {final_result}")
                
        except Exception as test_error:
            print(f"‚ö†Ô∏è TensorFlow test warning: {test_error}")
        
        print("‚úÖ TensorFlow intelligent configuration complete")
        return tf
        
    except ImportError as e:
        print(f"‚ùå TensorFlow import error: {e}")
        return None
    except Exception as e:
        print(f"‚ùå TensorFlow configuration error: {e}")
        # **Fallback: Return basic TensorFlow**
        try:
            import tensorflow as tf
            print("üîÑ Using basic TensorFlow configuration")
            return tf
        except:
            return None

# **GLOBAL: Initialize TensorFlow on import (if available)**
TF_AVAILABLE = is_tensorflow_available()
if TF_AVAILABLE:
    tf = get_tensorflow()
else:
    tf = None
    print("‚ö†Ô∏è TensorFlow not available - using CPU fallback mode") 