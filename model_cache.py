#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LSTM Model Cache Sistemi

Bu mod√ºl eƒüitilmi≈ü LSTM modellerini cache'leyerek:
- Daha √∂nce eƒüitilmi≈ü modelleri y√ºkler
- Incremental training (artƒ±rƒ±mlƒ± eƒüitim) yapar
- Model versiyonlarƒ±nƒ± takip eder
- Eƒüitim s√ºresini ve i≈ülem g√ºc√ºn√º optimize eder
"""

import os
import json
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Tuple
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

try:
    from lstm_model import CryptoLSTMModel
    from data_preprocessor import CryptoDataPreprocessor
except ImportError:
    print("‚ö†Ô∏è LSTM mod√ºl ve preprocessor gerekli")

class ModelCache:
    """
    LSTM model cache ve versiyonlama sistemi
    """
    
    def __init__(self, cache_dir: str = "model_cache"):
        """
        Model cache'ini ba≈ülatƒ±r
        
        Args:
            cache_dir (str): Cache dizini
        """
        self.cache_dir = cache_dir
        self.models_dir = os.path.join(cache_dir, "models")
        self.metadata_dir = os.path.join(cache_dir, "metadata")
        self.preprocessors_dir = os.path.join(cache_dir, "preprocessors")
        
        # Dizinleri olu≈ütur
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.metadata_dir, exist_ok=True)
        os.makedirs(self.preprocessors_dir, exist_ok=True)
        
        # Cache ayarlarƒ±
        self.max_model_age_days = 7  # Model maksimum 7 g√ºn ge√ßerli
        self.incremental_training_threshold = 0.85  # %85 accuracy altƒ±nda yeniden eƒüit
        self.max_cache_size = 50  # Maksimum cache'lenebilecek model sayƒ±sƒ±
        
        print(f"üì¶ Model Cache ba≈ülatƒ±ldƒ±: {cache_dir}")
    
    def _generate_model_id(self, symbol: str, config: Dict) -> str:
        """
        Model i√ßin benzersiz ID olu≈üturur
        
        Args:
            symbol (str): Trading √ßifti
            config (Dict): Model konfig√ºrasyonu
        
        Returns:
            str: Model ID
        """
        # Config'i string'e √ßevir ve hash olu≈ütur
        config_str = json.dumps(config, sort_keys=True)
        config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]
        
        return f"{symbol.replace('/', '_')}_{config_hash}"
    
    def _get_model_paths(self, model_id: str) -> Dict[str, str]:
        """
        Model dosya yollarƒ±nƒ± d√∂nd√ºr√ºr
        
        Args:
            model_id (str): Model ID
        
        Returns:
            Dict[str, str]: Dosya yollarƒ±
        """
        return {
            'model': os.path.join(self.models_dir, f"{model_id}.h5"),
            'weights': os.path.join(self.models_dir, f"{model_id}_weights.h5"),
            'metadata': os.path.join(self.metadata_dir, f"{model_id}.json"),
            'preprocessor': os.path.join(self.preprocessors_dir, f"{model_id}.pkl"),
            'scaler': os.path.join(self.preprocessors_dir, f"{model_id}_scaler.pkl")
        }
    
    def save_model(self, model: 'CryptoLSTMModel', preprocessor: 'CryptoDataPreprocessor',
                   symbol: str, config: Dict, training_data_info: Dict,
                   performance_metrics: Dict) -> str:
        """
        Modeli cache'e kaydeder
        
        Args:
            model: Eƒüitilmi≈ü LSTM modeli
            preprocessor: Veri √∂n i≈üleme sƒ±nƒ±fƒ±
            symbol: Trading √ßifti
            config: Model konfig√ºrasyonu
            training_data_info: Eƒüitim verisi bilgileri
            performance_metrics: Performans metrikleri
        
        Returns:
            str: Model ID
        """
        try:
            model_id = self._generate_model_id(symbol, config)
            paths = self._get_model_paths(model_id)
            
            # Model'i kaydet
            model.save_model(paths['model'])
            
            # Preprocessor'ƒ± kaydet
            with open(paths['preprocessor'], 'wb') as f:
                pickle.dump(preprocessor, f)
            
            # Scaler'ƒ± ayrƒ± kaydet
            with open(paths['scaler'], 'wb') as f:
                pickle.dump(preprocessor.scaler, f)
            
            # Metadata olu≈ütur
            metadata = {
                'model_id': model_id,
                'symbol': symbol,
                'config': config,
                'training_data_info': training_data_info,
                'performance_metrics': performance_metrics,
                'created_at': datetime.now().isoformat(),
                'last_trained': datetime.now().isoformat(),
                'training_count': 1,
                'feature_count': training_data_info.get('feature_count', 0),
                'data_hash': training_data_info.get('data_hash', ''),
                'version': '1.0'
            }
            
            # Metadata kaydet
            with open(paths['metadata'], 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Model cache'lendi: {model_id}")
            return model_id
            
        except Exception as e:
            print(f"‚ùå Model cache kaydetme hatasƒ±: {str(e)}")
            return None
    
    def load_model(self, model_id: str) -> Optional[Tuple['CryptoLSTMModel', 'CryptoDataPreprocessor', Dict]]:
        """
        Cache'den model y√ºkler
        
        Args:
            model_id (str): Model ID
        
        Returns:
            Optional[Tuple]: (model, preprocessor, metadata) veya None
        """
        try:
            paths = self._get_model_paths(model_id)
            
            # Dosyalarƒ±n varlƒ±ƒüƒ±nƒ± kontrol et
            if not all(os.path.exists(path) for path in [paths['model'], paths['metadata'], paths['preprocessor']]):
                return None
            
            # Metadata y√ºkle
            with open(paths['metadata'], 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # Model ya≈üƒ±nƒ± kontrol et
            last_trained = datetime.fromisoformat(metadata['last_trained'])
            if datetime.now() - last_trained > timedelta(days=self.max_model_age_days):
                print(f"‚ö†Ô∏è Model √ßok eski: {model_id} ({(datetime.now() - last_trained).days} g√ºn)")
                return None
            
            # Preprocessor y√ºkle
            with open(paths['preprocessor'], 'rb') as f:
                preprocessor = pickle.load(f)
            
            # Scaler y√ºkle (varsa)
            if os.path.exists(paths['scaler']):
                with open(paths['scaler'], 'rb') as f:
                    preprocessor.scaler = pickle.load(f)
            
            # Model y√ºkle
            config = metadata['config']
            sequence_length = config.get('sequence_length', 60)
            feature_count = metadata.get('feature_count', 10)
            
            model = CryptoLSTMModel(sequence_length, feature_count)
            model.load_model(paths['model'])
            
            print(f"üìÇ Model y√ºklendi: {model_id}")
            return model, preprocessor, metadata
            
        except Exception as e:
            print(f"‚ùå Model y√ºkleme hatasƒ±: {str(e)}")
            return None
    
    def find_cached_model(self, symbol: str, config: Dict) -> Optional[str]:
        """
        Belirli symbol ve config i√ßin cache'lenmi≈ü model arar
        
        Args:
            symbol (str): Trading √ßifti
            config (Dict): Model konfig√ºrasyonu
        
        Returns:
            Optional[str]: Model ID veya None
        """
        try:
            model_id = self._generate_model_id(symbol, config)
            paths = self._get_model_paths(model_id)
            
            if os.path.exists(paths['metadata']):
                with open(paths['metadata'], 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # Model ya≈üƒ±nƒ± kontrol et
                last_trained = datetime.fromisoformat(metadata['last_trained'])
                if datetime.now() - last_trained <= timedelta(days=self.max_model_age_days):
                    return model_id
            
            return None
            
        except Exception as e:
            print(f"‚ùå Model arama hatasƒ±: {str(e)}")
            return None
    
    def update_model(self, model_id: str, model: 'CryptoLSTMModel', 
                     new_performance: Dict, additional_data_info: Dict) -> bool:
        """
        Mevcut modeli incremental training ile g√ºnceller
        
        Args:
            model_id (str): Model ID
            model: G√ºncellenmi≈ü model
            new_performance: Yeni performans metrikleri
            additional_data_info: Ek veri bilgileri
        
        Returns:
            bool: G√ºncelleme ba≈üarƒ±sƒ±
        """
        try:
            paths = self._get_model_paths(model_id)
            
            # Mevcut metadata'yƒ± y√ºkle
            with open(paths['metadata'], 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # Model'i g√ºncelle
            model.save_model(paths['model'])
            
            # Metadata g√ºncelle
            metadata['last_trained'] = datetime.now().isoformat()
            metadata['training_count'] += 1
            metadata['performance_metrics'] = new_performance
            metadata['version'] = f"{metadata['training_count']}.0"
            
            # Ek veri bilgilerini ekle
            if 'training_history' not in metadata:
                metadata['training_history'] = []
            
            metadata['training_history'].append({
                'timestamp': datetime.now().isoformat(),
                'performance': new_performance,
                'data_info': additional_data_info
            })
            
            # Metadata kaydet
            with open(paths['metadata'], 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            print(f"üîÑ Model g√ºncellendi: {model_id} (v{metadata['version']})")
            return True
            
        except Exception as e:
            print(f"‚ùå Model g√ºncelleme hatasƒ±: {str(e)}")
            return False
    
    def should_retrain_model(self, metadata: Dict, current_performance: Optional[Dict] = None) -> bool:
        """
        Modelin yeniden eƒüitilip eƒüitilmeyeceƒüini belirler
        
        Args:
            metadata: Model metadata'sƒ±
            current_performance: Mevcut performans (varsa)
        
        Returns:
            bool: Yeniden eƒüitim gerekli mi?
        """
        try:
            # Ya≈ü kontrol√º
            last_trained = datetime.fromisoformat(metadata['last_trained'])
            age_days = (datetime.now() - last_trained).days
            
            if age_days > self.max_model_age_days:
                print(f"üîÑ Model √ßok eski, yeniden eƒüitim gerekli: {age_days} g√ºn")
                return True
            
            # Performans kontrol√º
            if current_performance:
                cached_accuracy = metadata.get('performance_metrics', {}).get('directional_accuracy', 0)
                current_accuracy = current_performance.get('directional_accuracy', 0)
                
                if current_accuracy < self.incremental_training_threshold:
                    print(f"üìâ D√º≈ü√ºk performans, yeniden eƒüitim gerekli: {current_accuracy:.1f}% < {self.incremental_training_threshold*100:.1f}%")
                    return True
                
                # Performans d√º≈üt√ºyse
                if current_accuracy < cached_accuracy * 0.9:  # %10 d√º≈ü√º≈ü
                    print(f"üìâ Performans d√º≈üt√º, yeniden eƒüitim gerekli: {current_accuracy:.1f}% -> {cached_accuracy:.1f}%")
                    return True
            
            return False
            
        except Exception as e:
            print(f"‚ùå Yeniden eƒüitim kontrol√º hatasƒ±: {str(e)}")
            return True  # Hata durumunda g√ºvenli tarafta kal
    
    def get_cache_stats(self) -> Dict:
        """
        Cache istatistiklerini d√∂nd√ºr√ºr
        
        Returns:
            Dict: Cache istatistikleri
        """
        try:
            stats = {
                'total_models': 0,
                'total_size_mb': 0,
                'models_by_symbol': {},
                'oldest_model': None,
                'newest_model': None,
                'cache_dir': self.cache_dir
            }
            
            model_dates = []
            
            for filename in os.listdir(self.metadata_dir):
                if filename.endswith('.json'):
                    filepath = os.path.join(self.metadata_dir, filename)
                    
                    with open(filepath, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    stats['total_models'] += 1
                    
                    symbol = metadata['symbol']
                    if symbol not in stats['models_by_symbol']:
                        stats['models_by_symbol'][symbol] = 0
                    stats['models_by_symbol'][symbol] += 1
                    
                    model_dates.append(datetime.fromisoformat(metadata['last_trained']))
            
            # Tarih istatistikleri
            if model_dates:
                stats['oldest_model'] = min(model_dates).isoformat()
                stats['newest_model'] = max(model_dates).isoformat()
            
            # Boyut hesapla
            total_size = 0
            for root, dirs, files in os.walk(self.cache_dir):
                for file in files:
                    filepath = os.path.join(root, file)
                    total_size += os.path.getsize(filepath)
            
            stats['total_size_mb'] = round(total_size / (1024 * 1024), 2)
            
            return stats
            
        except Exception as e:
            print(f"‚ùå Cache istatistik hatasƒ±: {str(e)}")
            return {}
    
    def cleanup_old_models(self, max_age_days: Optional[int] = None) -> int:
        """
        Eski modelleri temizler
        
        Args:
            max_age_days: Maksimum ya≈ü (varsayƒ±lan: self.max_model_age_days)
        
        Returns:
            int: Temizlenen model sayƒ±sƒ±
        """
        if max_age_days is None:
            max_age_days = self.max_model_age_days
        
        deleted_count = 0
        
        try:
            for filename in os.listdir(self.metadata_dir):
                if filename.endswith('.json'):
                    filepath = os.path.join(self.metadata_dir, filename)
                    
                    with open(filepath, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    last_trained = datetime.fromisoformat(metadata['last_trained'])
                    age_days = (datetime.now() - last_trained).days
                    
                    if age_days > max_age_days:
                        model_id = metadata['model_id']
                        
                        # ƒ∞lgili dosyalarƒ± sil
                        paths = self._get_model_paths(model_id)
                        for path in paths.values():
                            if os.path.exists(path):
                                os.remove(path)
                        
                        deleted_count += 1
                        print(f"üóëÔ∏è Eski model silindi: {model_id} ({age_days} g√ºn)")
            
            print(f"‚úÖ {deleted_count} eski model temizlendi")
            return deleted_count
            
        except Exception as e:
            print(f"‚ùå Model temizleme hatasƒ±: {str(e)}")
            return 0
    
    def list_cached_models(self) -> List[Dict]:
        """
        Cache'deki t√ºm modelleri listeler
        
        Returns:
            List[Dict]: Model listesi
        """
        models = []
        
        try:
            for filename in os.listdir(self.metadata_dir):
                if filename.endswith('.json'):
                    filepath = os.path.join(self.metadata_dir, filename)
                    
                    with open(filepath, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # Ya≈ü hesapla
                    last_trained = datetime.fromisoformat(metadata['last_trained'])
                    age_days = (datetime.now() - last_trained).days
                    
                    model_info = {
                        'model_id': metadata['model_id'],
                        'symbol': metadata['symbol'],
                        'version': metadata.get('version', '1.0'),
                        'last_trained': metadata['last_trained'],
                        'age_days': age_days,
                        'training_count': metadata.get('training_count', 1),
                        'performance': metadata.get('performance_metrics', {}),
                        'feature_count': metadata.get('feature_count', 0),
                        'valid': age_days <= self.max_model_age_days
                    }
                    
                    models.append(model_info)
            
            # Son eƒüitim tarihine g√∂re sƒ±rala
            models.sort(key=lambda x: x['last_trained'], reverse=True)
            
            return models
            
        except Exception as e:
            print(f"‚ùå Model listeleme hatasƒ±: {str(e)}")
            return []

class CachedModelManager:
    """
    Model cache'i kullanarak model eƒüitimi ve y√∂netimi yapar
    """
    
    def __init__(self, cache_dir: str = "model_cache"):
        """
        CachedModelManager'ƒ± ba≈ülatƒ±r
        
        Args:
            cache_dir (str): Cache dizini
        """
        self.cache = ModelCache(cache_dir)
        print("üß† Cached Model Manager ba≈ülatƒ±ldƒ±!")
    
    def get_or_train_model(self, symbol: str, data: pd.DataFrame, 
                          config: Dict, preprocessor: 'CryptoDataPreprocessor',
                          force_retrain: bool = False) -> Tuple['CryptoLSTMModel', 'CryptoDataPreprocessor', Dict]:
        """
        Cache'den model y√ºkler veya yeni eƒüitir
        
        Args:
            symbol (str): Trading √ßifti
            data (pd.DataFrame): Eƒüitim verisi
            config (Dict): Model konfig√ºrasyonu
            preprocessor (CryptoDataPreprocessor): Veri √∂n i≈üleme
            force_retrain (bool): Zorla yeniden eƒüitim
        
        Returns:
            Tuple: (model, preprocessor, training_info)
        """
        try:
            # Cache'de model ara
            model_id = self.cache.find_cached_model(symbol, config)
            
            if model_id and not force_retrain:
                print(f"üìÇ Cache'den model y√ºkleniyor: {symbol}")
                
                cached_result = self.cache.load_model(model_id)
                if cached_result:
                    model, cached_preprocessor, metadata = cached_result
                    
                    # Hƒ±zlƒ± performans testi
                    quick_test_result = self._quick_performance_test(model, cached_preprocessor, data)
                    
                    # Yeniden eƒüitim gerekli mi?
                    if not self.cache.should_retrain_model(metadata, quick_test_result):
                        print(f"‚úÖ Cache'den model kullanƒ±lƒ±yor: {symbol}")
                        return model, cached_preprocessor, metadata
                    else:
                        print(f"üîÑ Incremental training yapƒ±lƒ±yor: {symbol}")
                        return self._incremental_training(model_id, model, cached_preprocessor, data, config)
            
            # Yeni model eƒüit
            print(f"üß† Yeni model eƒüitiliyor: {symbol}")
            return self._train_new_model(symbol, data, config, preprocessor)
            
        except Exception as e:
            print(f"‚ùå Model alma/eƒüitme hatasƒ±: {str(e)}")
            # Fallback: Yeni model eƒüit
            return self._train_new_model(symbol, data, config, preprocessor)
    
    def _train_new_model(self, symbol: str, data: pd.DataFrame, 
                        config: Dict, preprocessor: 'CryptoDataPreprocessor') -> Tuple['CryptoLSTMModel', 'CryptoDataPreprocessor', Dict]:
        """
        Yeni model eƒüitir ve cache'ler
        """
        try:
            # Veri hazƒ±rla
            scaled_data = preprocessor.scale_data(data)
            X, y = preprocessor.create_sequences(scaled_data, config.get('sequence_length', 60))
            X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(X, y)
            
            # Model olu≈ütur ve eƒüit
            model = CryptoLSTMModel(config.get('sequence_length', 60), X_train.shape[2])
            model.build_model(
                config.get('lstm_units', [64, 64, 32]),
                config.get('dropout_rate', 0.3),
                config.get('learning_rate', 0.001)
            )
            
            # Eƒüitim
            history = model.train_model(
                X_train, y_train, X_val, y_val,
                epochs=config.get('epochs', 30),
                batch_size=config.get('batch_size', 32)
            )
            
            # Performans deƒüerlendirme
            metrics, predictions = model.evaluate_model(X_test, y_test)
            
            # Directional accuracy hesapla
            if len(y_test) > 1:
                y_test_direction = np.diff(y_test) > 0
                pred_direction = np.diff(predictions.flatten()) > 0
                directional_accuracy = np.mean(y_test_direction == pred_direction)
                metrics['directional_accuracy'] = directional_accuracy
            
            # Eƒüitim bilgileri
            training_data_info = {
                'data_shape': data.shape,
                'feature_count': X_train.shape[2],
                'training_samples': len(X_train),
                'data_hash': hashlib.md5(str(data.values.tobytes()).encode()).hexdigest()[:16]
            }
            
            # Cache'e kaydet
            model_id = self.cache.save_model(
                model, preprocessor, symbol, config, 
                training_data_info, metrics
            )
            
            return model, preprocessor, {
                'model_id': model_id,
                'training_type': 'new',
                'performance_metrics': metrics,
                'training_data_info': training_data_info
            }
            
        except Exception as e:
            print(f"‚ùå Yeni model eƒüitim hatasƒ±: {str(e)}")
            raise
    
    def _incremental_training(self, model_id: str, model: 'CryptoLSTMModel',
                            preprocessor: 'CryptoDataPreprocessor', new_data: pd.DataFrame,
                            config: Dict) -> Tuple['CryptoLSTMModel', 'CryptoDataPreprocessor', Dict]:
        """
        Mevcut model √ºzerinde incremental training yapar
        """
        try:
            print("üîÑ Incremental training ba≈ülƒ±yor...")
            
            # Yeni veri hazƒ±rla (cache'den y√ºklenen preprocessor i√ßin fit_scaler=False)
            scaled_data = preprocessor.scale_data(new_data, fit_scaler=False)
            X, y = preprocessor.create_sequences(scaled_data, config.get('sequence_length', 60))
            X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(X, y)
            
            # D√º≈ü√ºk learning rate ile eƒüitim
            incremental_lr = config.get('learning_rate', 0.001) * 0.1  # %10'u kadar
            
            # Model'in learning rate'ini g√ºncelle
            import tensorflow as tf
            tf.keras.backend.set_value(model.model.optimizer.learning_rate, incremental_lr)
            
            # Kƒ±sa eƒüitim (5-10 epoch)
            incremental_epochs = min(10, config.get('epochs', 30) // 3)
            
            # Incremental training i√ßin minimal callbacks
            from tensorflow.keras.callbacks import EarlyStopping
            incremental_callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=3,
                    restore_best_weights=True,
                    verbose=0
                )
            ]
            
            history = model.model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=incremental_epochs,
                batch_size=config.get('batch_size', 32),
                verbose=0,
                callbacks=incremental_callbacks
            )
            
            # Yeni performans deƒüerlendirme
            metrics, predictions = model.evaluate_model(X_test, y_test)
            
            # Directional accuracy hesapla
            if len(y_test) > 1:
                y_test_direction = np.diff(y_test) > 0
                pred_direction = np.diff(predictions.flatten()) > 0
                directional_accuracy = np.mean(y_test_direction == pred_direction)
                metrics['directional_accuracy'] = directional_accuracy
            
            # Ek veri bilgileri
            additional_data_info = {
                'incremental_training': True,
                'new_data_shape': new_data.shape,
                'incremental_epochs': incremental_epochs,
                'incremental_lr': incremental_lr
            }
            
            # Cache'i g√ºncelle
            self.cache.update_model(model_id, model, metrics, additional_data_info)
            
            print(f"‚úÖ Incremental training tamamlandƒ±: MSE {metrics['mse']:.6f}")
            
            return model, preprocessor, {
                'model_id': model_id,
                'training_type': 'incremental',
                'performance_metrics': metrics,
                'additional_data_info': additional_data_info
            }
            
        except Exception as e:
            print(f"‚ùå Incremental training hatasƒ±: {str(e)}")
            # Fallback: Yeni model eƒüit
            return self._train_new_model(symbol, new_data, config, preprocessor)
    
    def _quick_performance_test(self, model: 'CryptoLSTMModel', 
                               preprocessor: 'CryptoDataPreprocessor',
                               data: pd.DataFrame) -> Dict:
        """
        Modelin hƒ±zlƒ± performans testi
        """
        try:
            # Son %20'lik veri ile test
            test_size = int(len(data) * 0.2)
            test_data = data.tail(test_size)
            
            scaled_data = preprocessor.scale_data(test_data, fit_scaler=False)
            X, y = preprocessor.create_sequences(scaled_data, 60)
            
            if len(X) > 0:
                metrics, predictions = model.evaluate_model(X, y)
                
                # Directional accuracy hesapla
                if len(y) > 1:
                    y_direction = np.diff(y) > 0
                    pred_direction = np.diff(predictions.flatten()) > 0
                    directional_accuracy = np.mean(y_direction == pred_direction)
                    metrics['directional_accuracy'] = directional_accuracy
                
                return metrics
            
            return {'directional_accuracy': 0.5}  # Varsayƒ±lan
            
        except Exception as e:
            print(f"‚ö†Ô∏è Hƒ±zlƒ± test hatasƒ±: {str(e)}")
            return {'directional_accuracy': 0.5}
    
    def get_cache_info(self) -> Dict:
        """
        Cache bilgilerini d√∂nd√ºr√ºr
        """
        stats = self.cache.get_cache_stats()
        models = self.cache.list_cached_models()
        
        return {
            'cache_stats': stats,
            'cached_models': models,
            'valid_models': len([m for m in models if m['valid']]),
            'expired_models': len([m for m in models if not m['valid']])
        }
    
    def cleanup_cache(self) -> Dict:
        """
        Cache temizliƒüi yapar
        """
        deleted_count = self.cache.cleanup_old_models()
        
        return {
            'deleted_models': deleted_count,
            'cleanup_completed': True,
            'timestamp': datetime.now().isoformat()
        }
    
    def cleanup_old_models(self, days: int = 7):
        """
        Eski modelleri temizler (CachedModelManager i√ßin wrapper)
        
        Args:
            days (int): Ka√ß g√ºn √∂ncesine kadar temizlenecek
        """
        return self.cache.cleanup_old_models(days)

def main():
    """
    Model cache test fonksiyonu
    """
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                    ‚ïë
‚ïë                üß† LSTM MODEL CACHE Sƒ∞STEMƒ∞ üß†                     ‚ïë
‚ïë                                                                    ‚ïë
‚ïë  Bu sistem eƒüitilmi≈ü modelleri cache'leyerek eƒüitim s√ºresini      ‚ïë
‚ïë  ve i≈ülem g√ºc√ºn√º optimize eder.                                   ‚ïë
‚ïë                                                                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")
    
    # Cache manager olu≈ütur
    cache_manager = CachedModelManager()
    
    # Cache durumunu g√∂ster
    cache_info = cache_manager.get_cache_info()
    
    print(f"\nüìä CACHE DURUMU:")
    print(f"üíæ Toplam Model: {cache_info['cache_stats']['total_models']}")
    print(f"‚úÖ Ge√ßerli Model: {cache_info['valid_models']}")
    print(f"‚è∞ S√ºresi Dolmu≈ü: {cache_info['expired_models']}")
    print(f"üìÅ Cache Boyutu: {cache_info['cache_stats']['total_size_mb']} MB")
    
    if cache_info['cached_models']:
        print(f"\nüìã CACHE'DEKƒ∞ MODELLER:")
        for model in cache_info['cached_models'][:5]:  # ƒ∞lk 5'i g√∂ster
            status = "‚úÖ" if model['valid'] else "‚ùå"
            print(f"   {status} {model['symbol']} (v{model['version']}) - {model['age_days']} g√ºn")
    
    # Se√ßenekler
    print(f"\nüîß CACHE Y√ñNETƒ∞Mƒ∞:")
    print("1. üóëÔ∏è Eski modelleri temizle")
    print("2. üìä Detaylƒ± cache istatistikleri")
    print("3. üìã T√ºm modelleri listele")
    print("4. üö™ √áƒ±kƒ±≈ü")
    
    choice = input("\nSe√ßiminiz (1-4): ").strip()
    
    if choice == '1':
        result = cache_manager.cleanup_cache()
        print(f"‚úÖ {result['deleted_models']} model temizlendi")
    
    elif choice == '2':
        stats = cache_info['cache_stats']
        print(f"\nüìä DETAYLI ƒ∞STATƒ∞STƒ∞KLER:")
        print(f"Cache Dizini: {stats['cache_dir']}")
        print(f"Toplam Model: {stats['total_models']}")
        print(f"Cache Boyutu: {stats['total_size_mb']} MB")
        
        if stats['models_by_symbol']:
            print("\nSymbol Bazƒ±nda Daƒüƒ±lƒ±m:")
            for symbol, count in stats['models_by_symbol'].items():
                print(f"  {symbol}: {count} model")
    
    elif choice == '3':
        models = cache_info['cached_models']
        print(f"\nüìã T√úM MODELLER ({len(models)}):")
        
        for model in models:
            status = "‚úÖ Ge√ßerli" if model['valid'] else "‚ùå S√ºresi Dolmu≈ü"
            perf = model['performance'].get('directional_accuracy', 0)
            
            print(f"""
Model ID: {model['model_id']}
Symbol: {model['symbol']}
Versiyon: {model['version']}
Durum: {status}
Ya≈ü: {model['age_days']} g√ºn
Eƒüitim Sayƒ±sƒ±: {model['training_count']}
Performans: {perf:.1f}% accuracy
√ñzellik Sayƒ±sƒ±: {model['feature_count']}
""")

if __name__ == "__main__":
    main() 